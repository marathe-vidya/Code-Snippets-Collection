{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44db542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88ec056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
     ]
    }
   ],
   "source": [
    "# Reading string word by word\n",
    "introduction_text = ('This tutorial is about Natural Language Processing in Spacy.')\n",
    "introduction_doc = nlp(introduction_text)\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce723c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from file\n",
    "# >>> file_name = 'introduction.txt'\n",
    "# >>> introduction_file_text = open(file_name).read()\n",
    "# >>> introduction_file_doc = nlp(introduction_file_text)\n",
    "# >>> # Extract tokens for the given doc\n",
    "# >>> print ([token.text for token in introduction_file_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1037341e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of sentences\n",
    ">>> about_text = ('Let us look at this program. I am interested in learning Natural Language Processing.')\n",
    ">>> about_doc = nlp(about_text)\n",
    ">>> sentences = list(about_doc.sents)\n",
    ">>> len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2eebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Piano Academy 0 19 ORG Companies, agencies, institutions, etc.\n",
      "Mayfair 35 42 LOC Non-GPE locations, mountain ranges, bodies of water\n",
      "the City of London 46 64 GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    ">>> piano_class_text = ('Great Piano Academy is situated'\n",
    "...     ' in Mayfair or the City of London and has'\n",
    "...     ' world-class piano instructors.')\n",
    ">>> piano_class_doc = nlp(piano_class_text)\n",
    ">>> for ent in piano_class_doc.ents:\n",
    "...     print(ent.text, ent.start_char, ent.end_char,\n",
    "...           ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de005ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAS 8 11 ORG Companies, agencies, institutions, etc.\n",
      "Delhi India 20 31 ORG Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    ">>> piano_class_text = ('Get the SAS report, Delhi India.')\n",
    ">>> piano_class_doc = nlp(piano_class_text)\n",
    ">>> for ent in piano_class_doc.ents:\n",
    "...     print(ent.text, ent.start_char, ent.end_char,\n",
    "...           ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3686feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON      \t 4690420944186131903\tI\n",
      "am          AUX       \t10382539506755952630\tbe\n",
      "a           DET       \t11901859001352538922\ta\n",
      "runner      NOUN      \t12640964157389618806\trunner\n",
      "running     VERB      \t12767647472892411841\trun\n",
      "in          ADP       \t 3002984154512732771\tin\n",
      "a           DET       \t11901859001352538922\ta\n",
      "race        NOUN      \t 8048469955494714898\trace\n",
      "because     SCONJ     \t16950148841647037698\tbecause\n",
      "I           PRON      \t 4690420944186131903\tI\n",
      "love        VERB      \t 3702023516439754181\tlove\n",
      "to          PART      \t 3791531372978436496\tto\n",
      "run         VERB      \t12767647472892411841\trun\n",
      "since       SCONJ     \t10066841407251338481\tsince\n",
      "I           PRON      \t 4690420944186131903\tI\n",
      "ran         VERB      \t12767647472892411841\trun\n",
      "everyday    ADV       \t12502803309396265471\teveryday\n"
     ]
    }
   ],
   "source": [
    "text = nlp(u\"I am a runner running in a race because I love to run since I ran everyday\")\n",
    "\n",
    "for token in text:\n",
    "    print(f\"{token.text:{12}}{token.pos_:{10}}\\t{token.lemma:{20}}\\t{token.lemma_}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f202cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value [] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8698332283318978\n",
      "0.3559776908843717\n",
      "0.04888341581654538\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_md\n",
    "# Load a larger model with vectors\n",
    "nlpmd = en_core_web_md.load()\n",
    "\n",
    "# Compare two documents\n",
    "doc_1 = nlpmd(\"I like fast food\")\n",
    "doc_2 = nlpmd(\"I like pizza\")\n",
    "\n",
    "doc_3 = nlpmd(\"The weather is cold\")\n",
    "doc_4 = nlpmd(\"I like to drink hot coffee when its cold\")\n",
    "\n",
    "\n",
    "print(doc_1.similarity(doc_2))\n",
    "print(doc_4.similarity(doc_3))\n",
    "\n",
    "print(doc_2.similarity(doc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535a938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
